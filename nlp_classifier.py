# nlp_classifier.py
import pandas as pd
import numpy as np
import re
import logging
import pickle
from typing import Tuple, Optional, List, Dict

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report
from sklearn.preprocessing import LabelEncoder

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer

from config import TRAIN_DATA_PATH, MODEL_PATH, DEFAULT_CATEGORIES, CONFIDENCE_THRESHOLD

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
def download_nltk_resources():
    """–°–∫–∞—á–∏–≤–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã NLTK"""
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('corpora/stopwords')
    except LookupError:
        print("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK...")
        nltk.download('punkt')
        nltk.download('stopwords')
        print("–†–µ—Å—É—Ä—Å—ã NLTK –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
download_nltk_resources()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLTK –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
STOPWORDS_RU = set(stopwords.words('russian'))
STEMmer = SnowballStemmer('russian')

class TextPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞: –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä, —É–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not isinstance(text, str):
            return ""
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –∫—Ä–æ–º–µ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    @staticmethod
    def tokenize(text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        return word_tokenize(text, language='russian')
    
    @staticmethod
    def remove_stopwords(tokens: List[str]) -> List[str]:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤"""
        return [token for token in tokens if token not in STOPWORDS_RU]
    
    @staticmethod
    def stem_tokens(tokens: List[str]) -> List[str]:
        """–°—Ç–µ–º–º–∏–Ω–≥ —Ç–æ–∫–µ–Ω–æ–≤"""
        return [STEMmer.stem(token) for token in tokens]
    
    @staticmethod
    def preprocess(text: str) -> str:
        """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        # –û—á–∏—Å—Ç–∫–∞
        cleaned = TextPreprocessor.clean_text(text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = TextPreprocessor.tokenize(cleaned)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
        tokens = TextPreprocessor.remove_stopwords(tokens)
        
        # –°—Ç–µ–º–º–∏–Ω–≥
        tokens = TextPreprocessor.stem_tokens(tokens)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç—Ä–æ–∫—É
        return ' '.join(tokens)

class FinancialClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π"""
    
    def __init__(self):
        self.pipeline = None
        self.label_encoder = LabelEncoder()
        self.is_trained = False
        
    def load_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            df = pd.read_csv(TRAIN_DATA_PATH)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {TRAIN_DATA_PATH}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            if 'description' not in df.columns or 'category' not in df.columns:
                raise ValueError("CSV —Ñ–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ 'description' –∏ 'category'")
            
            # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            df = df.dropna(subset=['description', 'category'])
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
            df['description'] = df['description'].astype(str)
            df['category'] = df['category'].astype(str)
            
            return df['description'], df['category']
            
        except FileNotFoundError:
            logger.error(f"–§–∞–π–ª {TRAIN_DATA_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def create_pipeline(self) -> Pipeline:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        pipeline = Pipeline([
            ('tfidf', TfidfVectorizer(
                preprocessor=TextPreprocessor.preprocess,
                ngram_range=(1, 2),  # —É—á–∏—Ç—ã–≤–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –±–∏–≥—Ä–∞–º–º—ã
                max_features=1000,    # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á
                min_df=2,             # —Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∏–Ω–∏–º—É–º –≤ 2 –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
                max_df=0.8            # —Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –≤—Å—Ç—Ä–µ—á–∞—Ç—å—Å—è –º–∞–∫—Å–∏–º—É–º –≤ 80% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            )),
            ('classifier', LogisticRegression(
                max_iter=1000,
                random_state=42,
                multi_class='multinomial',
                solver='lbfgs',
                C=1.0
            ))
        ])
        return pipeline
    
    def train(self, test_size: float = 0.2) -> Dict[str, float]:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            X, y = self.load_data()
            
            # –ö–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            y_encoded = self.label_encoder.fit_transform(y)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
            X_train, X_test, y_train, y_test = train_test_split(
                X, y_encoded, test_size=test_size, random_state=42, stratify=y_encoded
            )
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
            self.pipeline = self.create_pipeline()
            self.pipeline.fit(X_train, y_train)
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            train_accuracy = self.pipeline.score(X_train, y_train)
            test_accuracy = self.pipeline.score(X_test, y_test)
            
            # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            y_pred = self.pipeline.predict(X_test)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            f1 = f1_score(y_test, y_pred, average='weighted')
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –æ—Ç—á–µ—Ç–∞
            y_test_decoded = self.label_encoder.inverse_transform(y_test)
            y_pred_decoded = self.label_encoder.inverse_transform(y_pred)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            report = classification_report(y_test_decoded, y_pred_decoded)
            
            logger.info(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
            logger.info(f"  Train accuracy: {train_accuracy:.4f}")
            logger.info(f"  Test accuracy: {test_accuracy:.4f}")
            logger.info(f"  F1-score: {f1:.4f}")
            logger.info(f"\nClassification Report:\n{report}")
            
            self.is_trained = True
            
            return {
                'train_accuracy': train_accuracy,
                'test_accuracy': test_accuracy,
                'f1_score': f1,
                'classification_report': report
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def predict(self, text: str, return_probability: bool = False) -> Tuple[Optional[str], Optional[float]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            text: –¢–µ–∫—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            return_probability: –í–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            
        Returns:
            –ï—Å–ª–∏ return_probability=False: (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
            –ï—Å–ª–∏ return_probability=True: (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –≤—Å–µ_–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)
        """
        if not self.is_trained or self.pipeline is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ train()")
        
        if not text or not isinstance(text, str):
            return (None, 0.0) if not return_probability else (None, 0.0, {})
        
        try:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            proba = self.pipeline.predict_proba([text])[0]
            
            # –ò–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            max_idx = np.argmax(proba)
            confidence = proba[max_idx]
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category = self.label_encoder.inverse_transform([max_idx])[0]
            
            if not return_probability:
                return category, confidence
            else:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
                all_categories = self.label_encoder.classes_
                probabilities = {cat: prob for cat, prob in zip(all_categories, proba)}
                return category, confidence, probabilities
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ '{text}': {e}")
            return (None, 0.0) if not return_probability else (None, 0.0, {})
    
    def predict_with_threshold(self, text: str, threshold: float = CONFIDENCE_THRESHOLD) -> Tuple[Optional[str], float, bool]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ—Ä–æ–≥–æ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏.
        
        Returns:
            (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, —É–≤–µ—Ä–µ–Ω–Ω–æ–µ_–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
            –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å < threshold, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (None, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, False)
        """
        category, confidence = self.predict(text)
        
        if confidence >= threshold:
            return category, confidence, True
        else:
            return None, confidence, False
    
    def save_model(self, filepath: str = MODEL_PATH):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª"""
        try:
            with open(filepath, 'wb') as f:
                pickle.dump({
                    'pipeline': self.pipeline,
                    'label_encoder': self.label_encoder,
                    'is_trained': self.is_trained
                }, f)
            logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def load_model(self, filepath: str = MODEL_PATH):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filepath, 'rb') as f:
                data = pickle.load(f)
            
            self.pipeline = data['pipeline']
            self.label_encoder = data['label_encoder']
            self.is_trained = data['is_trained']
            
            logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {filepath}")
            logger.info(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {list(self.label_encoder.classes_)}")
            
        except FileNotFoundError:
            logger.warning(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ù—É–∂–Ω–æ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å.")
            self.is_trained = False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            self.is_trained = False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
classifier = FinancialClassifier()

def initialize_classifier() -> FinancialClassifier:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (–æ–±—É—á–µ–Ω–∏–µ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞)"""
    global classifier
    
    # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    classifier.load_model()
    
    if not classifier.is_trained:
        print("–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
        metrics = classifier.train()
        classifier.save_model()
        
        print(f"\n‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞!")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {metrics['test_accuracy']:.2%}")
        print(f"   F1-score: {metrics['f1_score']:.2%}")
        if metrics['test_accuracy'] > 0.85:
            print("   üéØ –¶–µ–ª—å >85% –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞!")
        else:
            print("   ‚ö†Ô∏è  –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∏–∂–µ 85%. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö.")
    else:
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
    
    return classifier

def test_classifier_examples():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö"""
    clf = initialize_classifier()
    
    test_cases = [
        "–∫–æ—Ñ–µ –≤ —Å—Ç–∞—Ä–±–∞–∫—Å",
        "—Ç–∞–∫—Å–∏ –¥–æ —Ä–∞–±–æ—Ç—ã",
        "–ø—Ä–æ–¥—É–∫—Ç—ã –≤ –º–∞–≥–∞–∑–∏–Ω–µ",
        "–∫–∏–Ω–æ —Å –¥—Ä—É–∑—å—è–º–∏",
        "–æ–±–µ–¥ –≤ —Å—Ç–æ–ª–æ–≤–æ–π",
        "–ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –≤ –∞–ø—Ç–µ–∫–µ",
        "–∫—É—Ä—Å—ã –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ",
        "–ø–æ–¥–∞—Ä–æ–∫ –Ω–∞ –¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è",
        "–±–µ–Ω–∑–∏–Ω –Ω–∞ –∑–∞–ø—Ä–∞–≤–∫–µ",
        "—Å—Ç–æ–º–∞—Ç–æ–ª–æ–≥ –∑—É–±–Ω–æ–π",
    ]
    
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞:")
    print("-" * 50)
    
    for text in test_cases:
        category, confidence, is_confident = clf.predict_with_threshold(text)
        
        if is_confident:
            print(f"‚úÖ '{text}' ‚Üí {category} ({confidence:.2%})")
        else:
            print(f"‚ùì '{text}' ‚Üí –ù–ï–£–í–ï–†–ï–ù–ù–û ({confidence:.2%})")
    
    print("-" * 50)